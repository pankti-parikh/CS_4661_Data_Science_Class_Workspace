{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Question2: KNN Classification in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  A, B, C, D, F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer of A,B,C\n",
    "\n",
    "a- Read the iris dataset from the following URL: https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv and assign it to a Pandas DataFrame as you learned in tutorial Lab2-3.\n",
    "\n",
    "b- Split the dataset into testing and training sets with the following parameters: test_size=0.4, random_state=6\n",
    "\n",
    "c- Instantiate a KNN object with K=3, train it on the training set and test it on the testing set.Then, calculate the accuracy of your prediction as you learned in Lab3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score\n",
      "0.5\n",
      "Accuracy\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# The following line will import KNeighborsClassifier \"Class\"\n",
    "# KNeighborsClassifier is name of a \"sklearn class\" to perform \"KNN Classification\" \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Importing the required packages and libraries\n",
    "# we will need numpy and pandas later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# reading a CSV file directly from Web, and store it in a pandas DataFrame:\n",
    "# \"read_csv\" is a pandas function to read csv files from web or local device:\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')\n",
    "\n",
    "\n",
    "# Importing iris from sklearn embedded datasets\n",
    "# The following line only import the load_iris function from sklearn library. \n",
    "# This function can generate an object containing iris dataset \n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "# Running the sklearn function load_iris() to instantiate an \"object\" containing iris datset: \n",
    "iris = load_iris()\n",
    "\n",
    "# \"data\" attribute will return the iris dataset features:\n",
    "X = iris.data  # X will be feature matrix\n",
    "# print(X)\n",
    "\n",
    "# \"feature_names\" attribute will return the name of features:\n",
    "# print(iris.feature_names)\n",
    "\n",
    "# print(X.shape) # this line print the size of iris.data (iris feature matrix)\n",
    "\n",
    "# \"target\" attribute will return the iris dataset labels \n",
    "# for the sklearn embedded iris dataset, the labels are already converted to numeric\n",
    "y = iris.target  # y will be label vector\n",
    "# print(y)\n",
    "\n",
    "# print(y.shape) # this line print the size of iris.target\n",
    "\n",
    "# In the following line, \"knn\" is instantiated as an \"object\" of KNeighborsClassifier \"class\". \n",
    "k = 3\n",
    "my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "my_knn_for_cs4661.fit(X, y)\n",
    "X_Testing = [[6, 3, 5.9, 2.9]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "#print(y_predict)\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "# Two new data samples:\n",
    "X_Testing = [[6, 3, 5.9, 2.9],[3.2, 3, 1.9, 0.3]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "# print(y_predict)\n",
    "\n",
    "\n",
    "# Defining a function to convert \"categorical\" labels to \"numerical\" labels\n",
    "# This is optional, because the latest revision of sklearn accepts non-numerical labels too!\n",
    "def categorical_to_numeric(x):\n",
    "    if x == 'setosa':\n",
    "        return 0\n",
    "    elif x == 'versicolor':\n",
    "        return 1\n",
    "    elif x == 'virginica':\n",
    "        return 2\n",
    "\n",
    "    \n",
    "# Applying the function on species column and adding corrsponding numerical label column:\n",
    "iris_df['label'] = iris_df['species'].apply(categorical_to_numeric)\n",
    "# checking the dataset by printing every 10 lines:\n",
    "#iris_df[0::10]\n",
    "\n",
    "\n",
    "# Creating the Feature Matrix for iris dataset:\n",
    "# create a python list of feature names that would like to pick from the dataset:\n",
    "feature_cols = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "\n",
    "# use the above list to select the features from the original DataFrame\n",
    "X = iris_df[feature_cols]  \n",
    "\n",
    "# print the first 5 rows\n",
    "# X\n",
    "\n",
    "# checking the size of Feature Matix X:\n",
    "# print(X.shape)\n",
    "\n",
    "# select a Series of labels (the last column) from the DataFrame\n",
    "# y = iris_df['label'] # this is the index that we gave to the labels\n",
    "# OR:\n",
    "y = iris_df['species'] # this is the original categorical labels (the latest revision of sklearn accepts non-numerical labels)\n",
    "# checking the label vector by printing every 10 values\n",
    "# y[::10]\n",
    "\n",
    "\n",
    "# Randomly splitting the original dataset into training set and testing set\n",
    "# The function\"train_test_split\" from \"sklearn.cross_validation\" library performs random splitting.\n",
    "# \"test_size=0.4\" means that pick 40% of data samples for testing set, and the rest (60%) for training set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6)\n",
    "\n",
    "# print the size of the traning set:\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print the size of the testing set:\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "# Training ONLY on the training set:\n",
    "my_knn_for_cs4661.fit(X_train, y_train)\n",
    "# Testing on the testing set:\n",
    "y_predict = my_knn_for_cs4661.predict(X_test)\n",
    "# print(y_predict)\n",
    "\n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform element-to-element comparision and returns the \n",
    "# percent of correct predictions:\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Example:\n",
    "y_pred    = [0, 2, 1, 1]\n",
    "y_actual  = [0, 1, 2, 1]\n",
    "score = accuracy_score(y_actual, y_pred)\n",
    "\n",
    "print('Accuracy Score')\n",
    "print(score)\n",
    "\n",
    "\n",
    "# We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy \n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform the element-to-element comparision and returns the \n",
    "# portion of correct predictions:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print('Accuracy')\n",
    "print(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer of D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d- Repeat part (c) for K=1, K=5, K=7, K=11, K=15, K=27, K=59 (you can simply use a “for loop,”and save the final accuracy results in a list). Does the accuracy always get better byincreasing the value K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K value :\n",
      "[1, 3, 5, 7, 11, 13, 15, 27, 59]\n",
      "\n",
      "\n",
      "Accuracy Score : \n",
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "\n",
      "\n",
      "Accuracy : \n",
      "[0.95, 0.9666666666666667, 0.9833333333333333, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9333333333333333, 0.9166666666666666, 0.8166666666666667]\n"
     ]
    }
   ],
   "source": [
    "# The following line will import KNeighborsClassifier \"Class\"\n",
    "# KNeighborsClassifier is name of a \"sklearn class\" to perform \"KNN Classification\" \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Importing the required packages and libraries\n",
    "# we will need numpy and pandas later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# reading a CSV file directly from Web, and store it in a pandas DataFrame:\n",
    "# \"read_csv\" is a pandas function to read csv files from web or local device:\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')\n",
    "\n",
    "\n",
    "# Importing iris from sklearn embedded datasets\n",
    "# The following line only import the load_iris function from sklearn library. \n",
    "# This function can generate an object containing iris dataset \n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "# Running the sklearn function load_iris() to instantiate an \"object\" containing iris datset: \n",
    "iris = load_iris()\n",
    "\n",
    "# \"data\" attribute will return the iris dataset features:\n",
    "X = iris.data  # X will be feature matrix\n",
    "# print(X)\n",
    "\n",
    "# \"feature_names\" attribute will return the name of features:\n",
    "# print(iris.feature_names)\n",
    "\n",
    "# print(X.shape) # this line print the size of iris.data (iris feature matrix)\n",
    "\n",
    "# \"target\" attribute will return the iris dataset labels \n",
    "# for the sklearn embedded iris dataset, the labels are already converted to numeric\n",
    "y = iris.target  # y will be label vector\n",
    "# print(y)\n",
    "\n",
    "# print(y.shape) # this line print the size of iris.target\n",
    "\n",
    "# Defining a function to convert \"categorical\" labels to \"numerical\" labels\n",
    "# This is optional, because the latest revision of sklearn accepts non-numerical labels too!\n",
    "def categorical_to_numeric(x):\n",
    "    if x == 'setosa':\n",
    "        return 0\n",
    "    elif x == 'versicolor':\n",
    "        return 1\n",
    "    elif x == 'virginica':\n",
    "        return 2\n",
    "\n",
    "\n",
    "listK = [1,3,5,7,11,13,15,27,59,]\n",
    "ScoreK = []\n",
    "AccuracyK = []\n",
    "\n",
    "for k in listK:\n",
    "\n",
    "\n",
    "            # In the following line, \"knn\" is instantiated as an \"object\" of KNeighborsClassifier \"class\". \n",
    "            # k = 3\n",
    "            my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "\n",
    "            # We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "            my_knn_for_cs4661.fit(X, y)\n",
    "            X_Testing = [[6, 3, 5.9, 2.9]]\n",
    "            y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "            #print(y_predict)\n",
    "\n",
    "\n",
    "            # We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "            # Two new data samples:\n",
    "            X_Testing = [[6, 3, 5.9, 2.9],[3.2, 3, 1.9, 0.3]]\n",
    "            y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "            # print(y_predict)\n",
    "\n",
    "\n",
    "            # Applying the function on species column and adding corrsponding numerical label column:\n",
    "            iris_df['label'] = iris_df['species'].apply(categorical_to_numeric)\n",
    "            # checking the dataset by printing every 10 lines:\n",
    "            #iris_df[0::10]\n",
    "\n",
    "\n",
    "            # Creating the Feature Matrix for iris dataset:\n",
    "            # create a python list of feature names that would like to pick from the dataset:\n",
    "            feature_cols = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "\n",
    "            # use the above list to select the features from the original DataFrame\n",
    "            X = iris_df[feature_cols]  \n",
    "\n",
    "            # print the first 5 rows\n",
    "            # X\n",
    "\n",
    "            # checking the size of Feature Matix X:\n",
    "            # print(X.shape)\n",
    "\n",
    "            # select a Series of labels (the last column) from the DataFrame\n",
    "            # y = iris_df['label'] # this is the index that we gave to the labels\n",
    "            # OR:\n",
    "            y = iris_df['species'] # this is the original categorical labels (the latest revision of sklearn accepts non-numerical labels)\n",
    "            # checking the label vector by printing every 10 values\n",
    "            # y[::10]\n",
    "\n",
    "\n",
    "            # Randomly splitting the original dataset into training set and testing set\n",
    "            # The function\"train_test_split\" from \"sklearn.cross_validation\" library performs random splitting.\n",
    "            # \"test_size=0.4\" means that pick 40% of data samples for testing set, and the rest (60%) for training set.\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6)\n",
    "\n",
    "            # print the size of the traning set:\n",
    "            # print(X_train.shape)\n",
    "            # print(y_train.shape)\n",
    "            # print('\\n')\n",
    "\n",
    "\n",
    "            # print the size of the testing set:\n",
    "            # print(X_test.shape)\n",
    "            # print(y_test.shape)\n",
    "            # print('\\n')\n",
    "\n",
    "\n",
    "            # print(X_test)\n",
    "            # print(y_test)\n",
    "\n",
    "\n",
    "            # Training ONLY on the training set:\n",
    "            my_knn_for_cs4661.fit(X_train, y_train)\n",
    "            # Testing on the testing set:\n",
    "            y_predict = my_knn_for_cs4661.predict(X_test)\n",
    "            # print(y_predict)\n",
    "\n",
    "            # Function \"accuracy_score\" from \"sklearn.metrics\" will perform element-to-element comparision and returns the \n",
    "            # percent of correct predictions:\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            # Example:\n",
    "            y_pred    = [0, 2, 1, 1]\n",
    "            y_actual  = [0, 1, 2, 1]\n",
    "            score = accuracy_score(y_actual, y_pred)\n",
    "            ScoreK.append(score)\n",
    "\n",
    "\n",
    "            # We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy \n",
    "            # Function \"accuracy_score\" from \"sklearn.metrics\" will perform the element-to-element comparision and returns the \n",
    "            # portion of correct predictions:\n",
    "\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            accuracy = accuracy_score(y_test, y_predict)\n",
    "            AccuracyK.append(accuracy)\n",
    "            \n",
    "print( 'K value :')    \n",
    "print(listK)\n",
    "print('\\n')\n",
    "\n",
    "print('Accuracy Score : ')\n",
    "print(ScoreK)\n",
    "print('\\n')\n",
    "\n",
    "print('Accuracy : ')\n",
    "print(AccuracyK)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the accuracy always get better by increasing the value K?\n",
    " \n",
    "#  No, In this example when we increasing the value of K accuracy increasing certain value and after that it will start decreasing.\n",
    "\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer for E "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e-Now, suppose that we would like to make prediction based on only ONE single feature.To find the best single feature, we have to try every feature individually. In other word, we have to repeat part (c) with K=3, four times (each time using only one of the 4 features), and compute the accuracy each time. Then, check the accuracies.\n",
    "\n",
    "Which individual feature provide the best accuracy (the best feature)? Which one is the second best feature? (Note: you have to train, test, and evaluate your model 4 times, each time on a dataset including only one of the features, and save the final accuracy results in a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a list for the store the value of the each case with individual feauture\n",
    "feature_cols_list_accuracy = []\n",
    "feature_cols_list = ['sepal_length','sepal_width','petal_length','petal_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# case 1 : Feature 1 sepal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for feaute sepal_length\n",
      "0.7166666666666667\n"
     ]
    }
   ],
   "source": [
    "# The following line will import KNeighborsClassifier \"Class\"\n",
    "# KNeighborsClassifier is name of a \"sklearn class\" to perform \"KNN Classification\" \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Importing the required packages and libraries\n",
    "# we will need numpy and pandas later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# reading a CSV file directly from Web, and store it in a pandas DataFrame:\n",
    "# \"read_csv\" is a pandas function to read csv files from web or local device:\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')\n",
    "\n",
    "\n",
    "# Importing iris from sklearn embedded datasets\n",
    "# The following line only import the load_iris function from sklearn library. \n",
    "# This function can generate an object containing iris dataset \n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "# Running the sklearn function load_iris() to instantiate an \"object\" containing iris datset: \n",
    "iris = load_iris()\n",
    "\n",
    "# \"data\" attribute will return the iris dataset features:\n",
    "X = iris.data  # X will be feature matrix\n",
    "# print(X)\n",
    "\n",
    "# \"feature_names\" attribute will return the name of features:\n",
    "# print(iris.feature_names)\n",
    "\n",
    "# print(X.shape) # this line print the size of iris.data (iris feature matrix)\n",
    "\n",
    "# \"target\" attribute will return the iris dataset labels \n",
    "# for the sklearn embedded iris dataset, the labels are already converted to numeric\n",
    "y = iris.target  # y will be label vector\n",
    "# print(y)\n",
    "\n",
    "# print(y.shape) # this line print the size of iris.target\n",
    "\n",
    "# Defining a function to convert \"categorical\" labels to \"numerical\" labels\n",
    "# This is optional, because the latest revision of sklearn accepts non-numerical labels too!\n",
    "def categorical_to_numeric(x):\n",
    "    if x == 'setosa':\n",
    "        return 0\n",
    "    elif x == 'versicolor':\n",
    "        return 1\n",
    "    elif x == 'virginica':\n",
    "        return 2\n",
    "\n",
    "\n",
    "# In the following line, \"knn\" is instantiated as an \"object\" of KNeighborsClassifier \"class\". \n",
    "k = 3\n",
    "my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "my_knn_for_cs4661.fit(X, y)\n",
    "X_Testing = [[6, 3, 5.9, 2.9]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "#print(y_predict)\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "# Two new data samples:\n",
    "X_Testing = [[6, 3, 5.9, 2.9],[3.2, 3, 1.9, 0.3]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "# print(y_predict)\n",
    "\n",
    "# Applying the function on species column and adding corrsponding numerical label column:\n",
    "iris_df['label'] = iris_df['species'].apply(categorical_to_numeric)\n",
    "# checking the dataset by printing every 10 lines:\n",
    "#iris_df[0::10]\n",
    "\n",
    "\n",
    "# Creating the Feature Matrix for iris dataset:\n",
    "# create a python list of feature names that would like to pick from the dataset:\n",
    "# feature_cols= ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "feature_cols = ['sepal_length']\n",
    "\n",
    "# use the above list to select the features from the original DataFrame\n",
    "X = iris_df[feature_cols]  \n",
    "\n",
    "# print the first 5 rows\n",
    "# X\n",
    "\n",
    "# checking the size of Feature Matix X:\n",
    "# print(X.shape)\n",
    "\n",
    "# select a Series of labels (the last column) from the DataFrame\n",
    "# y = iris_df['label'] # this is the index that we gave to the labels\n",
    "# OR:\n",
    "y = iris_df['species'] # this is the original categorical labels (the latest revision of sklearn accepts non-numerical labels)\n",
    "# checking the label vector by printing every 10 values\n",
    "# y[::10]\n",
    "\n",
    "\n",
    "# Randomly splitting the original dataset into training set and testing set\n",
    "# The function\"train_test_split\" from \"sklearn.cross_validation\" library performs random splitting.\n",
    "# \"test_size=0.4\" means that pick 40% of data samples for testing set, and the rest (60%) for training set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6)\n",
    "\n",
    "# print the size of the traning set:\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print the size of the testing set:\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "# Training ONLY on the training set:\n",
    "my_knn_for_cs4661.fit(X_train, y_train)\n",
    "# Testing on the testing set:\n",
    "y_predict = my_knn_for_cs4661.predict(X_test)\n",
    "# print(y_predict)\n",
    "\n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform element-to-element comparision and returns the \n",
    "# percent of correct predictions:\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Example:\n",
    "y_pred    = [0, 2, 1, 1]\n",
    "y_actual  = [0, 1, 2, 1]\n",
    "score = accuracy_score(y_actual, y_pred)\n",
    "\n",
    "# print('Accuracy Score')\n",
    "# print(score)\n",
    "\n",
    "\n",
    "# We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy \n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform the element-to-element comparision and returns the \n",
    "# portion of correct predictions:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "feature_cols_list_accuracy.append(accuracy)\n",
    "print('Accuracy for feaute sepal_length')\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# case 2 : Feature 2 sepal_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for feaute sepal_width\n",
      "0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "# case 1 : Feature 1 sepal_length\n",
    "\n",
    "# The following line will import KNeighborsClassifier \"Class\"\n",
    "# KNeighborsClassifier is name of a \"sklearn class\" to perform \"KNN Classification\" \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Importing the required packages and libraries\n",
    "# we will need numpy and pandas later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# reading a CSV file directly from Web, and store it in a pandas DataFrame:\n",
    "# \"read_csv\" is a pandas function to read csv files from web or local device:\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')\n",
    "\n",
    "\n",
    "# Importing iris from sklearn embedded datasets\n",
    "# The following line only import the load_iris function from sklearn library. \n",
    "# This function can generate an object containing iris dataset \n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "# Running the sklearn function load_iris() to instantiate an \"object\" containing iris datset: \n",
    "iris = load_iris()\n",
    "\n",
    "# \"data\" attribute will return the iris dataset features:\n",
    "X = iris.data  # X will be feature matrix\n",
    "# print(X)\n",
    "\n",
    "# \"feature_names\" attribute will return the name of features:\n",
    "# print(iris.feature_names)\n",
    "\n",
    "# print(X.shape) # this line print the size of iris.data (iris feature matrix)\n",
    "\n",
    "# \"target\" attribute will return the iris dataset labels \n",
    "# for the sklearn embedded iris dataset, the labels are already converted to numeric\n",
    "y = iris.target  # y will be label vector\n",
    "# print(y)\n",
    "\n",
    "# print(y.shape) # this line print the size of iris.target\n",
    "\n",
    "# Defining a function to convert \"categorical\" labels to \"numerical\" labels\n",
    "# This is optional, because the latest revision of sklearn accepts non-numerical labels too!\n",
    "def categorical_to_numeric(x):\n",
    "    if x == 'setosa':\n",
    "        return 0\n",
    "    elif x == 'versicolor':\n",
    "        return 1\n",
    "    elif x == 'virginica':\n",
    "        return 2\n",
    "\n",
    "\n",
    "# In the following line, \"knn\" is instantiated as an \"object\" of KNeighborsClassifier \"class\". \n",
    "k = 3\n",
    "my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "my_knn_for_cs4661.fit(X, y)\n",
    "X_Testing = [[6, 3, 5.9, 2.9]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "#print(y_predict)\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "# Two new data samples:\n",
    "X_Testing = [[6, 3, 5.9, 2.9],[3.2, 3, 1.9, 0.3]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "# print(y_predict)\n",
    "\n",
    "# Applying the function on species column and adding corrsponding numerical label column:\n",
    "iris_df['label'] = iris_df['species'].apply(categorical_to_numeric)\n",
    "# checking the dataset by printing every 10 lines:\n",
    "#iris_df[0::10]\n",
    "\n",
    "\n",
    "# Creating the Feature Matrix for iris dataset:\n",
    "# create a python list of feature names that would like to pick from the dataset:\n",
    "# feature_cols= ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "feature_cols = ['sepal_width']\n",
    "\n",
    "# use the above list to select the features from the original DataFrame\n",
    "X = iris_df[feature_cols]  \n",
    "\n",
    "# print the first 5 rows\n",
    "# X\n",
    "\n",
    "# checking the size of Feature Matix X:\n",
    "# print(X.shape)\n",
    "\n",
    "# select a Series of labels (the last column) from the DataFrame\n",
    "# y = iris_df['label'] # this is the index that we gave to the labels\n",
    "# OR:\n",
    "y = iris_df['species'] # this is the original categorical labels (the latest revision of sklearn accepts non-numerical labels)\n",
    "# checking the label vector by printing every 10 values\n",
    "# y[::10]\n",
    "\n",
    "\n",
    "# Randomly splitting the original dataset into training set and testing set\n",
    "# The function\"train_test_split\" from \"sklearn.cross_validation\" library performs random splitting.\n",
    "# \"test_size=0.4\" means that pick 40% of data samples for testing set, and the rest (60%) for training set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6)\n",
    "\n",
    "# print the size of the traning set:\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print the size of the testing set:\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "# Training ONLY on the training set:\n",
    "my_knn_for_cs4661.fit(X_train, y_train)\n",
    "# Testing on the testing set:\n",
    "y_predict = my_knn_for_cs4661.predict(X_test)\n",
    "# print(y_predict)\n",
    "\n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform element-to-element comparision and returns the \n",
    "# percent of correct predictions:\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Example:\n",
    "y_pred    = [0, 2, 1, 1]\n",
    "y_actual  = [0, 1, 2, 1]\n",
    "score = accuracy_score(y_actual, y_pred)\n",
    "\n",
    "# print('Accuracy Score')\n",
    "# print(score)\n",
    "\n",
    "\n",
    "# We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy \n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform the element-to-element comparision and returns the \n",
    "# portion of correct predictions:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "feature_cols_list_accuracy.append(accuracy)\n",
    "print('Accuracy for feaute sepal_width')\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# case 3 : Feature 3 petal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for feaute petal_length\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# case 1 : Feature 1 sepal_length\n",
    "\n",
    "# The following line will import KNeighborsClassifier \"Class\"\n",
    "# KNeighborsClassifier is name of a \"sklearn class\" to perform \"KNN Classification\" \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Importing the required packages and libraries\n",
    "# we will need numpy and pandas later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# reading a CSV file directly from Web, and store it in a pandas DataFrame:\n",
    "# \"read_csv\" is a pandas function to read csv files from web or local device:\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')\n",
    "\n",
    "\n",
    "# Importing iris from sklearn embedded datasets\n",
    "# The following line only import the load_iris function from sklearn library. \n",
    "# This function can generate an object containing iris dataset \n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "# Running the sklearn function load_iris() to instantiate an \"object\" containing iris datset: \n",
    "iris = load_iris()\n",
    "\n",
    "# \"data\" attribute will return the iris dataset features:\n",
    "X = iris.data  # X will be feature matrix\n",
    "# print(X)\n",
    "\n",
    "# \"feature_names\" attribute will return the name of features:\n",
    "# print(iris.feature_names)\n",
    "\n",
    "# print(X.shape) # this line print the size of iris.data (iris feature matrix)\n",
    "\n",
    "# \"target\" attribute will return the iris dataset labels \n",
    "# for the sklearn embedded iris dataset, the labels are already converted to numeric\n",
    "y = iris.target  # y will be label vector\n",
    "# print(y)\n",
    "\n",
    "# print(y.shape) # this line print the size of iris.target\n",
    "\n",
    "# Defining a function to convert \"categorical\" labels to \"numerical\" labels\n",
    "# This is optional, because the latest revision of sklearn accepts non-numerical labels too!\n",
    "def categorical_to_numeric(x):\n",
    "    if x == 'setosa':\n",
    "        return 0\n",
    "    elif x == 'versicolor':\n",
    "        return 1\n",
    "    elif x == 'virginica':\n",
    "        return 2\n",
    "\n",
    "\n",
    "# In the following line, \"knn\" is instantiated as an \"object\" of KNeighborsClassifier \"class\". \n",
    "k = 3\n",
    "my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "my_knn_for_cs4661.fit(X, y)\n",
    "X_Testing = [[6, 3, 5.9, 2.9]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "#print(y_predict)\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "# Two new data samples:\n",
    "X_Testing = [[6, 3, 5.9, 2.9],[3.2, 3, 1.9, 0.3]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "# print(y_predict)\n",
    "\n",
    "# Applying the function on species column and adding corrsponding numerical label column:\n",
    "iris_df['label'] = iris_df['species'].apply(categorical_to_numeric)\n",
    "# checking the dataset by printing every 10 lines:\n",
    "#iris_df[0::10]\n",
    "\n",
    "\n",
    "# Creating the Feature Matrix for iris dataset:\n",
    "# create a python list of feature names that would like to pick from the dataset:\n",
    "# feature_cols= ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "feature_cols = ['petal_length']\n",
    "\n",
    "# use the above list to select the features from the original DataFrame\n",
    "X = iris_df[feature_cols]  \n",
    "\n",
    "# print the first 5 rows\n",
    "# X\n",
    "\n",
    "# checking the size of Feature Matix X:\n",
    "# print(X.shape)\n",
    "\n",
    "# select a Series of labels (the last column) from the DataFrame\n",
    "# y = iris_df['label'] # this is the index that we gave to the labels\n",
    "# OR:\n",
    "y = iris_df['species'] # this is the original categorical labels (the latest revision of sklearn accepts non-numerical labels)\n",
    "# checking the label vector by printing every 10 values\n",
    "# y[::10]\n",
    "\n",
    "\n",
    "# Randomly splitting the original dataset into training set and testing set\n",
    "# The function\"train_test_split\" from \"sklearn.cross_validation\" library performs random splitting.\n",
    "# \"test_size=0.4\" means that pick 40% of data samples for testing set, and the rest (60%) for training set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6)\n",
    "\n",
    "# print the size of the traning set:\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print the size of the testing set:\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "# Training ONLY on the training set:\n",
    "my_knn_for_cs4661.fit(X_train, y_train)\n",
    "# Testing on the testing set:\n",
    "y_predict = my_knn_for_cs4661.predict(X_test)\n",
    "# print(y_predict)\n",
    "\n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform element-to-element comparision and returns the \n",
    "# percent of correct predictions:\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Example:\n",
    "y_pred    = [0, 2, 1, 1]\n",
    "y_actual  = [0, 1, 2, 1]\n",
    "score = accuracy_score(y_actual, y_pred)\n",
    "\n",
    "# print('Accuracy Score')\n",
    "# print(score)\n",
    "\n",
    "\n",
    "# We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy \n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform the element-to-element comparision and returns the \n",
    "# portion of correct predictions:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "feature_cols_list_accuracy.append(accuracy)\n",
    "print('Accuracy for feaute petal_length')\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# case 4 : Feature 4 petal_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for feaute petal_width\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "# case 1 : Feature 1 sepal_length\n",
    "\n",
    "# The following line will import KNeighborsClassifier \"Class\"\n",
    "# KNeighborsClassifier is name of a \"sklearn class\" to perform \"KNN Classification\" \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Importing the required packages and libraries\n",
    "# we will need numpy and pandas later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# reading a CSV file directly from Web, and store it in a pandas DataFrame:\n",
    "# \"read_csv\" is a pandas function to read csv files from web or local device:\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')\n",
    "\n",
    "\n",
    "# Importing iris from sklearn embedded datasets\n",
    "# The following line only import the load_iris function from sklearn library. \n",
    "# This function can generate an object containing iris dataset \n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "# Running the sklearn function load_iris() to instantiate an \"object\" containing iris datset: \n",
    "iris = load_iris()\n",
    "\n",
    "# \"data\" attribute will return the iris dataset features:\n",
    "X = iris.data  # X will be feature matrix\n",
    "# print(X)\n",
    "\n",
    "# \"feature_names\" attribute will return the name of features:\n",
    "# print(iris.feature_names)\n",
    "\n",
    "# print(X.shape) # this line print the size of iris.data (iris feature matrix)\n",
    "\n",
    "# \"target\" attribute will return the iris dataset labels \n",
    "# for the sklearn embedded iris dataset, the labels are already converted to numeric\n",
    "y = iris.target  # y will be label vector\n",
    "# print(y)\n",
    "\n",
    "# print(y.shape) # this line print the size of iris.target\n",
    "\n",
    "# Defining a function to convert \"categorical\" labels to \"numerical\" labels\n",
    "# This is optional, because the latest revision of sklearn accepts non-numerical labels too!\n",
    "def categorical_to_numeric(x):\n",
    "    if x == 'setosa':\n",
    "        return 0\n",
    "    elif x == 'versicolor':\n",
    "        return 1\n",
    "    elif x == 'virginica':\n",
    "        return 2\n",
    "\n",
    "\n",
    "# In the following line, \"knn\" is instantiated as an \"object\" of KNeighborsClassifier \"class\". \n",
    "k = 3\n",
    "my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "my_knn_for_cs4661.fit(X, y)\n",
    "X_Testing = [[6, 3, 5.9, 2.9]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "#print(y_predict)\n",
    "\n",
    "\n",
    "# We can use the method \"predict\" of the *trained* object knn on one or more testing data sample to perform prediction:\n",
    "# Two new data samples:\n",
    "X_Testing = [[6, 3, 5.9, 2.9],[3.2, 3, 1.9, 0.3]]\n",
    "y_predict = my_knn_for_cs4661.predict(X_Testing)\n",
    "# print(y_predict)\n",
    "\n",
    "# Applying the function on species column and adding corrsponding numerical label column:\n",
    "iris_df['label'] = iris_df['species'].apply(categorical_to_numeric)\n",
    "# checking the dataset by printing every 10 lines:\n",
    "#iris_df[0::10]\n",
    "\n",
    "\n",
    "# Creating the Feature Matrix for iris dataset:\n",
    "# create a python list of feature names that would like to pick from the dataset:\n",
    "# feature_cols= ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "feature_cols = ['petal_width']\n",
    "\n",
    "# use the above list to select the features from the original DataFrame\n",
    "X = iris_df[feature_cols]  \n",
    "\n",
    "# print the first 5 rows\n",
    "# X\n",
    "\n",
    "# checking the size of Feature Matix X:\n",
    "# print(X.shape)\n",
    "\n",
    "# select a Series of labels (the last column) from the DataFrame\n",
    "# y = iris_df['label'] # this is the index that we gave to the labels\n",
    "# OR:\n",
    "y = iris_df['species'] # this is the original categorical labels (the latest revision of sklearn accepts non-numerical labels)\n",
    "# checking the label vector by printing every 10 values\n",
    "# y[::10]\n",
    "\n",
    "\n",
    "# Randomly splitting the original dataset into training set and testing set\n",
    "# The function\"train_test_split\" from \"sklearn.cross_validation\" library performs random splitting.\n",
    "# \"test_size=0.4\" means that pick 40% of data samples for testing set, and the rest (60%) for training set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6)\n",
    "\n",
    "# print the size of the traning set:\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print the size of the testing set:\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "# Training ONLY on the training set:\n",
    "my_knn_for_cs4661.fit(X_train, y_train)\n",
    "# Testing on the testing set:\n",
    "y_predict = my_knn_for_cs4661.predict(X_test)\n",
    "# print(y_predict)\n",
    "\n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform element-to-element comparision and returns the \n",
    "# percent of correct predictions:\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Example:\n",
    "y_pred    = [0, 2, 1, 1]\n",
    "y_actual  = [0, 1, 2, 1]\n",
    "score = accuracy_score(y_actual, y_pred)\n",
    "\n",
    "# print('Accuracy Score')\n",
    "# print(score)\n",
    "\n",
    "\n",
    "# We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy \n",
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform the element-to-element comparision and returns the \n",
    "# portion of correct predictions:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "feature_cols_list_accuracy.append(accuracy)\n",
    "print('Accuracy for feaute petal_width')\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "[0.7166666666666667, 0.5666666666666667, 0.9333333333333333, 0.95]\n"
     ]
    }
   ],
   "source": [
    "print(feature_cols_list)\n",
    "print(feature_cols_list_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Which individual feature provide the best accuracy (the best feature)? Which one is the second best feature? (Note: you have to train, test, and evaluate your model 4 times, each time on a dataset including only one of the features, and save the final accuracy results in a list).\n",
    "\n",
    "# 1st best feature is petal_width\n",
    "\n",
    "# 2nd best feature is peatl_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
